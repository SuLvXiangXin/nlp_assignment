{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-01\n",
    "顾淳 19307110344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "from nltk import sent_tokenize, word_tokenize, ngrams, FreqDist, ConditionalFreqDist\n",
    "import warnings \n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "wiki_data = []\n",
    "with open(\"enwiki_20220201.json\",\"r\") as f:\n",
    "    for each_line in f:\n",
    "        record = json.loads(each_line)\n",
    "        wiki_data.append(record)\n",
    "print(len(wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#wiki_data=wiki_data[:10]+wiki_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据导入 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen_Kane</td>\n",
       "      <td>Film</td>\n",
       "      <td>Citizen Kane is a 1941 American drama film pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It_(2017_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>It, titled on-screen as It: Chapter One, is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star_Wars_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Wars (retroactively titled Star Wars: Epi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Star_Trek:_The_Motion_Picture</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Trek: The Motion Picture is a 1979 Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frozen_(2013_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Frozen is a 2013 American computer-animated mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black_Panther_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Black Panther is a 2018 American superhero fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The_Cabinet_of_Dr._Caligari</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Cabinet of Dr. Caligari () is a 1920 Germa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The_Shining_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Shining is a 1980 psychological horror fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scream_(franchise)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Scream is an American meta horror slasher medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Batman_v_Superman:_Dawn_of_Justice</td>\n",
       "      <td>Film</td>\n",
       "      <td>Batman v Superman: Dawn of Justice is a 2016 A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title label  \\\n",
       "0                        Citizen_Kane  Film   \n",
       "1                      It_(2017_film)  Film   \n",
       "2                    Star_Wars_(film)  Film   \n",
       "3       Star_Trek:_The_Motion_Picture  Film   \n",
       "4                  Frozen_(2013_film)  Film   \n",
       "5                Black_Panther_(film)  Film   \n",
       "6         The_Cabinet_of_Dr._Caligari  Film   \n",
       "7                  The_Shining_(film)  Film   \n",
       "8                  Scream_(franchise)  Film   \n",
       "9  Batman_v_Superman:_Dawn_of_Justice  Film   \n",
       "\n",
       "                                                text  \n",
       "0  Citizen Kane is a 1941 American drama film pro...  \n",
       "1  It, titled on-screen as It: Chapter One, is a ...  \n",
       "2  Star Wars (retroactively titled Star Wars: Epi...  \n",
       "3  Star Trek: The Motion Picture is a 1979 Americ...  \n",
       "4  Frozen is a 2013 American computer-animated mu...  \n",
       "5  Black Panther is a 2018 American superhero fil...  \n",
       "6  The Cabinet of Dr. Caligari () is a 1920 Germa...  \n",
       "7  The Shining is a 1980 psychological horror fil...  \n",
       "8  Scream is an American meta horror slasher medi...  \n",
       "9  Batman v Superman: Dawn of Justice is a 2016 A...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wiki_data).iloc[:10]\n",
    "# 删除 wiki_data 释放内存\n",
    "del wiki_data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对所有 text 进行分句、分词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\PyCharm 2020.1.2\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_utils.py\", line 69, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen_Kane</td>\n",
       "      <td>Film</td>\n",
       "      <td>Citizen Kane is a 1941 American drama film pro...</td>\n",
       "      <td>[Citizen Kane is a 1941 American drama film pr...</td>\n",
       "      <td>776</td>\n",
       "      <td>[[Citizen, Kane, is, a, 1941, American, drama,...</td>\n",
       "      <td>20011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It_(2017_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>It, titled on-screen as It: Chapter One, is a ...</td>\n",
       "      <td>[It, titled on-screen as It: Chapter One, is a...</td>\n",
       "      <td>597</td>\n",
       "      <td>[[It, ,, titled, on-screen, as, It, :, Chapter...</td>\n",
       "      <td>19044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star_Wars_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Wars (retroactively titled Star Wars: Epi...</td>\n",
       "      <td>[Star Wars (retroactively titled Star Wars: Ep...</td>\n",
       "      <td>643</td>\n",
       "      <td>[[Star, Wars, (, retroactively, titled, Star, ...</td>\n",
       "      <td>18572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Star_Trek:_The_Motion_Picture</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Trek: The Motion Picture is a 1979 Americ...</td>\n",
       "      <td>[Star Trek: The Motion Picture is a 1979 Ameri...</td>\n",
       "      <td>657</td>\n",
       "      <td>[[Star, Trek, :, The, Motion, Picture, is, a, ...</td>\n",
       "      <td>17179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frozen_(2013_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Frozen is a 2013 American computer-animated mu...</td>\n",
       "      <td>[Frozen is a 2013 American computer-animated m...</td>\n",
       "      <td>550</td>\n",
       "      <td>[[Frozen, is, a, 2013, American, computer-anim...</td>\n",
       "      <td>16584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black_Panther_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Black Panther is a 2018 American superhero fil...</td>\n",
       "      <td>[Black Panther is a 2018 American superhero fi...</td>\n",
       "      <td>509</td>\n",
       "      <td>[[Black, Panther, is, a, 2018, American, super...</td>\n",
       "      <td>15312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The_Cabinet_of_Dr._Caligari</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Cabinet of Dr. Caligari () is a 1920 Germa...</td>\n",
       "      <td>[The Cabinet of Dr. Caligari () is a 1920 Germ...</td>\n",
       "      <td>498</td>\n",
       "      <td>[[The, Cabinet, of, Dr., Caligari, (, ), is, a...</td>\n",
       "      <td>14763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The_Shining_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Shining is a 1980 psychological horror fil...</td>\n",
       "      <td>[The Shining is a 1980 psychological horror fi...</td>\n",
       "      <td>541</td>\n",
       "      <td>[[The, Shining, is, a, 1980, psychological, ho...</td>\n",
       "      <td>15022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scream_(franchise)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Scream is an American meta horror slasher medi...</td>\n",
       "      <td>[Scream is an American meta horror slasher med...</td>\n",
       "      <td>458</td>\n",
       "      <td>[[Scream, is, an, American, meta, horror, slas...</td>\n",
       "      <td>14758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Batman_v_Superman:_Dawn_of_Justice</td>\n",
       "      <td>Film</td>\n",
       "      <td>Batman v Superman: Dawn of Justice is a 2016 A...</td>\n",
       "      <td>[Batman v Superman: Dawn of Justice is a 2016 ...</td>\n",
       "      <td>513</td>\n",
       "      <td>[[Batman, v, Superman, :, Dawn, of, Justice, i...</td>\n",
       "      <td>14625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title label  \\\n",
       "0                        Citizen_Kane  Film   \n",
       "1                      It_(2017_film)  Film   \n",
       "2                    Star_Wars_(film)  Film   \n",
       "3       Star_Trek:_The_Motion_Picture  Film   \n",
       "4                  Frozen_(2013_film)  Film   \n",
       "5                Black_Panther_(film)  Film   \n",
       "6         The_Cabinet_of_Dr._Caligari  Film   \n",
       "7                  The_Shining_(film)  Film   \n",
       "8                  Scream_(franchise)  Film   \n",
       "9  Batman_v_Superman:_Dawn_of_Justice  Film   \n",
       "\n",
       "                                                text  \\\n",
       "0  Citizen Kane is a 1941 American drama film pro...   \n",
       "1  It, titled on-screen as It: Chapter One, is a ...   \n",
       "2  Star Wars (retroactively titled Star Wars: Epi...   \n",
       "3  Star Trek: The Motion Picture is a 1979 Americ...   \n",
       "4  Frozen is a 2013 American computer-animated mu...   \n",
       "5  Black Panther is a 2018 American superhero fil...   \n",
       "6  The Cabinet of Dr. Caligari () is a 1920 Germa...   \n",
       "7  The Shining is a 1980 psychological horror fil...   \n",
       "8  Scream is an American meta horror slasher medi...   \n",
       "9  Batman v Superman: Dawn of Justice is a 2016 A...   \n",
       "\n",
       "                                           sentences  num_sentences  \\\n",
       "0  [Citizen Kane is a 1941 American drama film pr...            776   \n",
       "1  [It, titled on-screen as It: Chapter One, is a...            597   \n",
       "2  [Star Wars (retroactively titled Star Wars: Ep...            643   \n",
       "3  [Star Trek: The Motion Picture is a 1979 Ameri...            657   \n",
       "4  [Frozen is a 2013 American computer-animated m...            550   \n",
       "5  [Black Panther is a 2018 American superhero fi...            509   \n",
       "6  [The Cabinet of Dr. Caligari () is a 1920 Germ...            498   \n",
       "7  [The Shining is a 1980 psychological horror fi...            541   \n",
       "8  [Scream is an American meta horror slasher med...            458   \n",
       "9  [Batman v Superman: Dawn of Justice is a 2016 ...            513   \n",
       "\n",
       "                                               words  num_words  \n",
       "0  [[Citizen, Kane, is, a, 1941, American, drama,...      20011  \n",
       "1  [[It, ,, titled, on-screen, as, It, :, Chapter...      19044  \n",
       "2  [[Star, Wars, (, retroactively, titled, Star, ...      18572  \n",
       "3  [[Star, Trek, :, The, Motion, Picture, is, a, ...      17179  \n",
       "4  [[Frozen, is, a, 2013, American, computer-anim...      16584  \n",
       "5  [[Black, Panther, is, a, 2018, American, super...      15312  \n",
       "6  [[The, Cabinet, of, Dr., Caligari, (, ), is, a...      14763  \n",
       "7  [[The, Shining, is, a, 1980, psychological, ho...      15022  \n",
       "8  [[Scream, is, an, American, meta, horror, slas...      14758  \n",
       "9  [[Batman, v, Superman, :, Dawn, of, Justice, i...      14625  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 sent_tokenize 函数对 text 进行分句\n",
    "df['sentences'] = df['text'].apply(lambda t: sent_tokenize(t))\n",
    "# 计算各 text 的句子数\n",
    "df['num_sentences'] = df['sentences'].apply(lambda t: len(t))\n",
    "\n",
    "# 使用 word_tokenize 函数对 sentence 进行分词\n",
    "df['words'] = df['sentences'].apply(lambda t: [word_tokenize(sent) for sent in t if sent])\n",
    "# 计算各 text 的单词数\n",
    "df['num_words'] = df['words'].apply(lambda t: sum([len(sent) for sent in t]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示各 label 的数量、平均句数、平均单词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sentences_average</th>\n",
       "      <th>words_average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>10</td>\n",
       "      <td>574.2</td>\n",
       "      <td>16587.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  sentences_average  words_average\n",
       "label                                         \n",
       "Film      10              574.2        16587.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算各 label 对应的文件数量\n",
    "label_count = df['label'].groupby(df['label']).count()\n",
    "# 计算平均句子数\n",
    "sentences_average = df['num_sentences'].groupby(df['label']).mean()\n",
    "# 计算平均单词数\n",
    "words_average = df['num_words'].groupby(df['label']).mean()\n",
    "# 将上述结果拼接方便查看\n",
    "labels = pd.concat(\n",
    "    {'count': label_count, 'sentences_average': sentences_average.round(2), 'words_average': words_average.round(2)},\n",
    "    axis=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义预处理函数，对句子进行去特殊字符以及小写化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "raw string:\n",
      " Citizen Kane is a 1941 American drama film produced by, directed by, and starring Orson Welles. \n",
      "\n",
      "preprocessing:\n",
      " citizen kane is a 1941 american drama film produced by  directed by  and starring orson welles \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # 将句内除字母、数字、空格外的所有字符替换为空格\n",
    "    res = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    return res.lower()\n",
    "\n",
    "# 用一个示例查看 preprocessing 函数的作用\n",
    "print('Example:\\n')\n",
    "print('raw string:\\n', df['sentences'][0][0], '\\n')\n",
    "print('preprocessing:\\n', preprocess(df['sentences'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用预处理函数更新 dataset 中的 sentences 和 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen_Kane</td>\n",
       "      <td>Film</td>\n",
       "      <td>Citizen Kane is a 1941 American drama film pro...</td>\n",
       "      <td>[Citizen Kane is a 1941 American drama film pr...</td>\n",
       "      <td>776</td>\n",
       "      <td>20011</td>\n",
       "      <td>[[Citizen, Kane, is, a, 1941, American, drama,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It_(2017_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>It, titled on-screen as It: Chapter One, is a ...</td>\n",
       "      <td>[It, titled on-screen as It: Chapter One, is a...</td>\n",
       "      <td>597</td>\n",
       "      <td>19044</td>\n",
       "      <td>[[It, ,, titled, on-screen, as, It, :, Chapter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star_Wars_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Wars (retroactively titled Star Wars: Epi...</td>\n",
       "      <td>[Star Wars (retroactively titled Star Wars: Ep...</td>\n",
       "      <td>643</td>\n",
       "      <td>18572</td>\n",
       "      <td>[[Star, Wars, (, retroactively, titled, Star, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Star_Trek:_The_Motion_Picture</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Trek: The Motion Picture is a 1979 Americ...</td>\n",
       "      <td>[Star Trek: The Motion Picture is a 1979 Ameri...</td>\n",
       "      <td>657</td>\n",
       "      <td>17179</td>\n",
       "      <td>[[Star, Trek, :, The, Motion, Picture, is, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frozen_(2013_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Frozen is a 2013 American computer-animated mu...</td>\n",
       "      <td>[Frozen is a 2013 American computer-animated m...</td>\n",
       "      <td>550</td>\n",
       "      <td>16584</td>\n",
       "      <td>[[Frozen, is, a, 2013, American, computer-anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black_Panther_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Black Panther is a 2018 American superhero fil...</td>\n",
       "      <td>[Black Panther is a 2018 American superhero fi...</td>\n",
       "      <td>509</td>\n",
       "      <td>15312</td>\n",
       "      <td>[[Black, Panther, is, a, 2018, American, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The_Cabinet_of_Dr._Caligari</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Cabinet of Dr. Caligari () is a 1920 Germa...</td>\n",
       "      <td>[The Cabinet of Dr. Caligari () is a 1920 Germ...</td>\n",
       "      <td>498</td>\n",
       "      <td>14763</td>\n",
       "      <td>[[The, Cabinet, of, Dr., Caligari, (, ), is, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The_Shining_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>The Shining is a 1980 psychological horror fil...</td>\n",
       "      <td>[The Shining is a 1980 psychological horror fi...</td>\n",
       "      <td>541</td>\n",
       "      <td>15022</td>\n",
       "      <td>[[The, Shining, is, a, 1980, psychological, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scream_(franchise)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Scream is an American meta horror slasher medi...</td>\n",
       "      <td>[Scream is an American meta horror slasher med...</td>\n",
       "      <td>458</td>\n",
       "      <td>14758</td>\n",
       "      <td>[[Scream, is, an, American, meta, horror, slas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Batman_v_Superman:_Dawn_of_Justice</td>\n",
       "      <td>Film</td>\n",
       "      <td>Batman v Superman: Dawn of Justice is a 2016 A...</td>\n",
       "      <td>[Batman v Superman: Dawn of Justice is a 2016 ...</td>\n",
       "      <td>513</td>\n",
       "      <td>14625</td>\n",
       "      <td>[[Batman, v, Superman, :, Dawn, of, Justice, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title label  \\\n",
       "0                        Citizen_Kane  Film   \n",
       "1                      It_(2017_film)  Film   \n",
       "2                    Star_Wars_(film)  Film   \n",
       "3       Star_Trek:_The_Motion_Picture  Film   \n",
       "4                  Frozen_(2013_film)  Film   \n",
       "5                Black_Panther_(film)  Film   \n",
       "6         The_Cabinet_of_Dr._Caligari  Film   \n",
       "7                  The_Shining_(film)  Film   \n",
       "8                  Scream_(franchise)  Film   \n",
       "9  Batman_v_Superman:_Dawn_of_Justice  Film   \n",
       "\n",
       "                                                text  \\\n",
       "0  Citizen Kane is a 1941 American drama film pro...   \n",
       "1  It, titled on-screen as It: Chapter One, is a ...   \n",
       "2  Star Wars (retroactively titled Star Wars: Epi...   \n",
       "3  Star Trek: The Motion Picture is a 1979 Americ...   \n",
       "4  Frozen is a 2013 American computer-animated mu...   \n",
       "5  Black Panther is a 2018 American superhero fil...   \n",
       "6  The Cabinet of Dr. Caligari () is a 1920 Germa...   \n",
       "7  The Shining is a 1980 psychological horror fil...   \n",
       "8  Scream is an American meta horror slasher medi...   \n",
       "9  Batman v Superman: Dawn of Justice is a 2016 A...   \n",
       "\n",
       "                                           sentences  num_sentences  \\\n",
       "0  [Citizen Kane is a 1941 American drama film pr...            776   \n",
       "1  [It, titled on-screen as It: Chapter One, is a...            597   \n",
       "2  [Star Wars (retroactively titled Star Wars: Ep...            643   \n",
       "3  [Star Trek: The Motion Picture is a 1979 Ameri...            657   \n",
       "4  [Frozen is a 2013 American computer-animated m...            550   \n",
       "5  [Black Panther is a 2018 American superhero fi...            509   \n",
       "6  [The Cabinet of Dr. Caligari () is a 1920 Germ...            498   \n",
       "7  [The Shining is a 1980 psychological horror fi...            541   \n",
       "8  [Scream is an American meta horror slasher med...            458   \n",
       "9  [Batman v Superman: Dawn of Justice is a 2016 ...            513   \n",
       "\n",
       "   num_words                                              words  \n",
       "0      20011  [[Citizen, Kane, is, a, 1941, American, drama,...  \n",
       "1      19044  [[It, ,, titled, on-screen, as, It, :, Chapter...  \n",
       "2      18572  [[Star, Wars, (, retroactively, titled, Star, ...  \n",
       "3      17179  [[Star, Trek, :, The, Motion, Picture, is, a, ...  \n",
       "4      16584  [[Frozen, is, a, 2013, American, computer-anim...  \n",
       "5      15312  [[Black, Panther, is, a, 2018, American, super...  \n",
       "6      14763  [[The, Cabinet, of, Dr., Caligari, (, ), is, a...  \n",
       "7      15022  [[The, Shining, is, a, 1980, psychological, ho...  \n",
       "8      14758  [[Scream, is, an, American, meta, horror, slas...  \n",
       "9      14625  [[Batman, v, Superman, :, Dawn, of, Justice, i...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除空词\n",
    "def del_blank(sent):\n",
    "    try:\n",
    "        while True:\n",
    "            sent.remove('')\n",
    "    except:\n",
    "        return sent\n",
    "# 用 preprocessing 函数去除句内特殊字符，更新其他项。\n",
    "# df['sentences'] = df['sentences'].apply(lambda t:[preprocess(sent) for sent in t])\n",
    "# df['num_sentences'] = df['sentences'].apply(lambda t:len(t))\n",
    "# 先删除此列节省内存\n",
    "del df['words']\n",
    "df['words'] = df['sentences'].apply(lambda t: [del_blank(word_tokenize(preprocess(sent))) for sent in t])\n",
    "df['num_words'] = df['words'].apply(lambda t: sum([len(sent) for sent in t]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15995\n"
     ]
    }
   ],
   "source": [
    "# 得到词汇表，防止在后续过程中出现未知单词\n",
    "vocab = set()\n",
    "for text in df['words']:\n",
    "    for sent in text:\n",
    "        vocab.update(set(sent))\n",
    "vocab.update({'<end>','<begin>','<end>'})\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据集随机分为90%训练集和10%测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\PyCharm 2020.1.2\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_utils.py\", line 69, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_set: 9\n",
      "Length of test_set: 1\n"
     ]
    }
   ],
   "source": [
    "# 设置 90% 的训练集\n",
    "np.random.seed(0)\n",
    "train_rate = 0.9\n",
    "arr = np.arange(len(df))\n",
    "\n",
    "# 随机打乱 arr 顺序，以达到打乱训练集和测试集的效果\n",
    "np.random.shuffle(arr)\n",
    "train_set = df['words'].iloc[arr[:int(len(arr) * train_rate)]]\n",
    "test_set = df['words'].iloc[arr[int(len(arr) * train_rate):]]\n",
    "print(\"Length of train_set:\", len(train_set))\n",
    "print(\"Length of test_set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 n-gram 语言模型\n",
    "在 ngrams_model 类中，定义了 add-one 和 kenser-ney 两种平滑函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ngrams_model:\n",
    "    def __init__(self, n, method, vocab):\n",
    "        self.n = n\n",
    "        # 预定义 self.dic 中的每个 value 均为 ConditionalFreqDist 类， self.dic 第一层选择 n-gram（应对需要回退或者插值的情况），\n",
    "        # 第二层选择 context，第三层选择 word\n",
    "        self.prefix = defaultdict(ConditionalFreqDist)\n",
    "        # unigram 不需要 context\n",
    "        self.uni = FreqDist()\n",
    "        self.vocab = vocab\n",
    "        if method in ['add_one', 'kneser_ney']:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise 'Method is restricted in `Add-one` or `Kneser-Ney`'\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(sent, k):\n",
    "        if k==1:\n",
    "            return sent+['<end>']\n",
    "        pad1 = ['_'] * (k - 1) + ['<begin>']\n",
    "        pad2 = ['<end>']\n",
    "        return pad1 + sent + pad2\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # 得到各个长度的语言模型上的 context 和 word 的对应关系\n",
    "        if self.method == 'add_one':\n",
    "            k = self.n\n",
    "            for text in dataset:\n",
    "                for sent in text:\n",
    "                    # 在每句话前后加 padding\n",
    "                    sent = ngrams_model.pad(sent, k)\n",
    "                    for i in range(len(sent) - k + 1):\n",
    "                        gram = tuple(sent[i:i + k])\n",
    "                        if k == 1:\n",
    "                            self.uni[gram[0]] += 1\n",
    "                        else:\n",
    "                            self.prefix[k][gram[:-1]][gram[-1]] += 1\n",
    "        elif self.method == 'kneser_ney':\n",
    "            self.suffix = defaultdict(ConditionalFreqDist)\n",
    "            #self.lam_q = defaultdict(ConditionalFreqDist)\n",
    "            self.total_gram=defaultdict(int)\n",
    "            self.middle=defaultdict(ConditionalFreqDist)\n",
    "            for k in range(1, self.n + 2):\n",
    "                for text in dataset:\n",
    "                    for sent in text:\n",
    "                        # 在每句话前后加 padding\n",
    "                        sent = ngrams_model.pad(sent, k)\n",
    "                        for i in range(len(sent) - k + 1):\n",
    "                            gram = tuple(sent[i:i + k])\n",
    "                            if k == 1:\n",
    "                                self.uni[gram[0]] += 1\n",
    "                            else:\n",
    "                                self.prefix[k][gram[:-1]][gram[-1]] += 1\n",
    "                                self.suffix[k][gram[1:]][gram[0]] += 1\n",
    "                            if k>=3:\n",
    "                                self.middle[k][gram[1:-1]][(gram[0],gram[-1])]=1\n",
    "                                #self.lam_q[k] = sum(1 for w in self.prefix[k][context] if self.suffix[n + 1][tuple(list(context) + [w])].N())\n",
    "                self.total_gram[k] = sum(dic.B() for dic in self.prefix[k].values())\n",
    "            #self.total_bigram = len(self.vocab)**2\n",
    "\n",
    "    def add_one(self, context, word):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            dic = self.uni\n",
    "        else:\n",
    "            dic = self.prefix[n][context]\n",
    "        # 进行 add-1 平滑\n",
    "        up = dic[word] + 1\n",
    "        down = dic.N() + len(self.vocab)\n",
    "        return up / down\n",
    "\n",
    "    def kneser_ney(self, context, word, d=0.1):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            # P_continuation()\n",
    "            up = self.suffix[2][(word,)].B()\n",
    "            down = self.total_gram[2]\n",
    "            res = (up + 1) / (down + len(self.vocab))\n",
    "            return res\n",
    "        if n == self.n:\n",
    "            dic = self.prefix[n][context]\n",
    "            if dic[word]:\n",
    "                a = max(0, dic[word] - d) / dic.N()\n",
    "            else:\n",
    "                a = 0\n",
    "            if dic.B() == 0:\n",
    "                lam = 1\n",
    "            else:\n",
    "                lam = d * dic.B() / dic.N()\n",
    "        else:\n",
    "            # 简化后续代码\n",
    "            # Continuation count(x)= Number of unique single word contexts for x\n",
    "            context_plus = tuple(list(context) + [word])\n",
    "            c_kn_up = self.suffix[n + 1][context_plus].B()\n",
    "            c_kn_down = self.total_gram[n+2]\n",
    "            if c_kn_up:\n",
    "                a = max(0, c_kn_up - d) / c_kn_down\n",
    "            else:\n",
    "                a = 0\n",
    "            q = self.middle[n+1][context[1:]].N()\n",
    "            #q_ = sum(1 for w in self.prefix[n][context] if self.suffix[n + 1][tuple(list(context) + [w])].B())\n",
    "            #print(q,q_)\n",
    "            if q == 0:\n",
    "                lam = 1\n",
    "            else:\n",
    "                lam = d*q/c_kn_down\n",
    "        # 计算 lambda\n",
    "\n",
    "\n",
    "        return a + lam * self.kneser_ney(context[1:], word, d)  # 递归\n",
    "\n",
    "    def perplexity(self, sent):\n",
    "        # 计算困惑度\n",
    "        # 如果输入为 string，对其进行 preprocessing 以及分词\n",
    "        if isinstance(sent, str):\n",
    "            sent = word_tokenize(preprocess(sent))\n",
    "        sent_ = ngrams_model.pad(sent, self.n)\n",
    "        ngrams_ = [tuple(sent_[i:i + self.n]) for i in range(len(sent_) - self.n + 1)]\n",
    "        # 计算 log probability，防止数据下溢\n",
    "        log_prob = -sum([log(getattr(self, self.method)(gram[:-1], gram[-1]), 2) for gram in ngrams_]) / len(sent)\n",
    "        return pow(log_prob, 2)  # 由 log probabilitu 转换回 probability\n",
    "\n",
    "    def test(self, dataset):\n",
    "        # 计算测试集各句子平均困惑度\n",
    "        res = []\n",
    "        for text in dataset:\n",
    "            for sent in text:\n",
    "                res.append(self.perplexity(sent))\n",
    "        return sum(res) / len(res)\n",
    "\n",
    "    def generate(self, early_stop=None):\n",
    "        # 生成句子\n",
    "        # 设置初始 context\n",
    "        context = tuple(['_'] * (self.n - 2) + ['<begin>'])[:self.n - 1]\n",
    "        gram = '<begin>'\n",
    "        sent = []\n",
    "        keys = list(self.vocab)\n",
    "        i = 0\n",
    "        while gram != '<end>':\n",
    "            # 获得一个 [0,1] 的概率\n",
    "            p = np.random.rand()\n",
    "            # 获得词汇表中所有词的概率 pdf\n",
    "            pdf = []\n",
    "            for key in keys:\n",
    "                pdf.append(getattr(self, self.method)(context, key))\n",
    "            # 累加得到 cdf\n",
    "            cdf = np.array(pdf).cumsum()\n",
    "            # 找到随机概率对应的 word\n",
    "            gram = keys[cdf.searchsorted(p)]\n",
    "            #print(gram, end=' ')\n",
    "            sent.append(gram)\n",
    "            context = tuple(list(context[1:]) + [gram])\n",
    "            i += 1\n",
    "            # 是否需要提前停止\n",
    "            if early_stop is not None and i >= early_stop:\n",
    "                sent.append('<end>')\n",
    "                break\n",
    "        # 去除最后的 <end> \n",
    "        sent = sent[:-1]\n",
    "        return sent\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # 可直接调用模型计算单词概率\n",
    "        return getattr(self, self.method)(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数验证概率和是否为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prob_sum(model):\n",
    "    # 随意选择的 context\n",
    "    context = tuple(['_'] * (model.n - 2) + ['<begin>'])[:model.n - 1]\n",
    "    l = []\n",
    "    for key in model.vocab:\n",
    "        l.append(model(context, key))\n",
    "    print('%d-gram language model using %s smoothing: P(w|context) add up to' % (model.n, model.method), sum(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到 unigram, bigram, trigram 语言模型，使用 Add-one 或 Kneser-Ney 两种平滑函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Program Files (x86)\\PyCharm 2020.1.2\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_utils.py\", line 69, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mine mine mine mine mine mine mine mine wizened Player P. mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine delays mine mine mine mine mine mine mine mine Madison mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Amazonian mine pursues mine mine mine mine mine Of mine mine mine mine mine mine theater mine mine mine mine Funko mine mine mine mine mine mine mine mine mine suddenly mine before mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Daisy mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Corporation mine mine mine mine mine mine read mine mine mine mine Grimm mine hibernation mine mine mine mine mine immersive mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine eponym mine mine mine mine mine 2,057 hierarchy mine mine mine mine mine mine mine mine properly mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Stendhal mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine dwarfs mine mine mine mine mine mine muscle mine mine mine mine mine Moff mine mine mine mine mine mine mine mine mine mine mine mine mine mine elevators mine mine mine mine mine mine mine mine mine mine mine mine Titan mine mine mine mine mine mine explains mine mine mine expensive mine network mine mine heart—that 12million mine mine mine mine mine mine mine mine chess mine mine mine mine mine mine mine mine mine crouching mine mine mine mine mine mine mine mine mine mine Steadfast mine Responding mine mine mine mine mine novelization mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine headless mine mine mine north mine mine mine undertones mine mine mine L. mine mine mine mine mine mine ARP vessels mine glasses mine mine mine mine psychologically mine mine mine mine mine mine mine mine mine showing mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine knights mine mine mine mine mine mine mine mine mine mine mine mine mine funding mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Mullally mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Green mine mine mine mine mine mine mine mine mine mine mine mine mine mine Irma mine Visitors mine mine mine sensitivity mine familiarity mine mine mine mine mine mine mine mine mine prolongs mine mine mine mine asserted mine mine mine mine mine Sheri mine mine mine daughters mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Release mine mine mine mine mine mine mine mine mine mine mine recognizable mine mine Similar mitzvah Brooklyn mine mine mine mine mine mine mine mine mine mine mine mine mine mine Northumberland mine mine mine mine mine mine mine mine mine mine Pow mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine anime mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine tenor mine mine mine mine mine unnatural mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Angeles mine mine mine mine mine mine mine mine summer mine mine mine mine mine mine mine mine Bloch mine mine mine mine mine canceled mine mine mine mine Belgian Pictures mine mine mine DuVernay indecision mine mine mine mine mine mine mine mine mine mine mine mine mine Anton mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine plywood mine authentic mine mine theatrical mine mine mine mine mine mine mine mine mine mine mine strained mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine Timbuktu mine mine mine mine mine mine mine mine mine mine mine mine Bonnie mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine mine "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m results[(method,n)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_perplexity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest(test_set)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 使用模型生成 5 个句子\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m results[(method,n)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerate_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m[model\u001b[38;5;241m.\u001b[39mgenerate() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n\u001b[0;32m     12\u001b[0m prob_sum(model)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 删除 model 释放内存\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#del model\u001b[39;00m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m results[(method,n)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_perplexity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest(test_set)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 使用模型生成 5 个句子\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m results[(method,n)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerate_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m[\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n\u001b[0;32m     12\u001b[0m prob_sum(model)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 删除 model 释放内存\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#del model\u001b[39;00m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mngrams_model.generate\u001b[1;34m(self, early_stop)\u001b[0m\n\u001b[0;32m    125\u001b[0m pdf\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[1;32m--> 127\u001b[0m     pdf\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# 累加得到 cdf\u001b[39;00m\n\u001b[0;32m    129\u001b[0m cdf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pdf)\u001b[38;5;241m.\u001b[39mcumsum()\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mngrams_model.kneser_ney\u001b[1;34m(self, context, word, d)\u001b[0m\n\u001b[0;32m     59\u001b[0m pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic[n][context]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre\u001b[38;5;241m.\u001b[39mB()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# 处理未知词汇\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m d\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)  \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 分情况计算 C_{kn}\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Highest order\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 将结果保存子 rusults 字典中\n",
    "results = defaultdict(dict)\n",
    "for method in ['add_one','kneser_ney']:\n",
    "    for n in [1,2,3]:\n",
    "        model = ngrams_model(n, method, vocab)\n",
    "        model.fit(train_set)\n",
    "        # 计算模型在测试集上的困惑度\n",
    "        results[(method,n)]['test_perplexity']=model.test(test_set)\n",
    "        # 使用模型生成 5 个句子\n",
    "        prob_sum(model)\n",
    "        results[(method, n)]['generate_sentences'] = [model.generate() for i in range(5)]\n",
    "        # 删除 model 释放内存\n",
    "        del model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看各 model 在测试集所有句子上的平均困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(\n",
    "    [results[(method, n)]['test_perplexity'] for method in ['add_one', 'kneser_ney'] for n in [1, 2, 3]],\n",
    "    index=[(method, n) for method in ['add_one', 'kneser_ney'] for n in [1, 2, 3]], columns=['Perplexity'])\n",
    "frame.columns.name = ('method', 'n-gram')\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看各 model 生成的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for method in ['add_one', 'kneser_ney']:\n",
    "    for n in [1, 2, 3]:\n",
    "        print('%d-gram language model using %s smoothing.' % (n, method))\n",
    "        for i in range(5):\n",
    "            print('\\t%s' % results[(method, n)]['generate_sentences'][i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 NaiveBayes 模型\n",
    "在 NaiveBayes 类中，实现了计算任意输入 label 的概率，任意输入词相对于任意输入 label 的条件概率，以及在测试集上的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self,vocab,alpha=1):\n",
    "        # 定义 self.dic 为 ConditionalFreqDist 类，第一层选择 label，第二层选择 word\n",
    "        self.dic=ConditionalFreqDist()\n",
    "        # 定义 self.label_freq 为 FreqDist 类, 用于统计 label 词频\n",
    "        self.label_freq=FreqDist()\n",
    "        # Laplace 平滑系数\n",
    "        self.alpha=alpha\n",
    "    def fit(self,train_set):\n",
    "        # 统计各条件概率和先验概率\n",
    "        for text_idx in train_set.index:\n",
    "            label = train_set.loc[text_idx,'label']\n",
    "            self.label_freq[label]+=1\n",
    "            for sent in train_set.loc[text_idx,'words']:\n",
    "                for word in sent:\n",
    "                    self.dic[label][word]+=1\n",
    "                    \n",
    "    def calculate(self,label,word=None):\n",
    "        # 计算单个条件概率或先验概率\n",
    "        if word is None:\n",
    "            # P(label)，不进行平滑\n",
    "            up = self.label_freq[label]\n",
    "            down = self.label_freq.N()\n",
    "            return up/down\n",
    "        else:\n",
    "            # P(word|label)，进行 Laplace 平滑\n",
    "            up = self.dic[label][word]\n",
    "            down = self.dic[label].N()\n",
    "            return (up+self.alpha)/(down+self.alpha*len(vocab))\n",
    "        \n",
    "    def __call__(self,text):\n",
    "        # 计算给定文本为各个 label 的概率\n",
    "        # 假如输入数据不符合格式，对其进行处理\n",
    "        if isinstance(text, str):\n",
    "            text = word_tokenize(preprocess(text))\n",
    "        elif isinstance(text[0], list):\n",
    "            text = [word for sent in text for word in sent]\n",
    "        res={}\n",
    "        for label in self.label_freq.keys():\n",
    "            # 防止数字下溢，使用 log probability\n",
    "            log_p=log(self.calculate(label),2)\n",
    "            for word in text:\n",
    "                log_p+=log(self.calculate(label,word),2)\n",
    "            res[label]=log_p\n",
    "        # 对 log probability 进行排序，最大可能性的 label 即为预测结果\n",
    "        log_sorted=sorted(res.items(),reverse=True,key=lambda x:x[1])\n",
    "        pred = log_sorted[0][0]\n",
    "        return {'prediction':pred,'log_prob':log_sorted}\n",
    "    \n",
    "    def test(self,test_set,beta=1):\n",
    "        # 在验证集上进行评估\n",
    "        # 总 label 数\n",
    "        n=self.label_freq.B()\n",
    "        # label 列表\n",
    "        labels=list(self.label_freq.keys())\n",
    "        # 测试集文本数\n",
    "        sum_=len(test_set.index)\n",
    "        # 总混淆矩阵\n",
    "        confusion_matrix=pd.DataFrame(np.zeros((n,n)),index=labels,columns=labels)\n",
    "        # 各类别的混淆矩阵\n",
    "        confusion_matrix_each_class=dict()\n",
    "        # 统计每次预测结果，记录于总混淆矩阵\n",
    "        for text_idx in test_set.index:\n",
    "            label = test_set.loc[text_idx,'label']\n",
    "            pred = self.__call__(test_set.loc[text_idx,'words'])['prediction']\n",
    "            confusion_matrix.loc[pred,label]+=1\n",
    "        confusion_matrix=confusion_matrix.astype('int64')\n",
    "        # 计算各类别混淆矩阵\n",
    "        for label in labels:\n",
    "            tp=confusion_matrix.loc[label,label]\n",
    "            fp=confusion_matrix.loc[label].sum()-tp\n",
    "            fn=confusion_matrix.loc[:,label].sum()-tp\n",
    "            tn=sum_-tp-fp-fn\n",
    "            confusion_matrix_each_class[label]=pd.DataFrame([[tp,fp],[fn,tn]],index=['+','-'],columns=['+','-']).astype('int64')\n",
    "        # 防止出现除以 0 出现 nan\n",
    "        def na2zero(x):\n",
    "            if np.isnan(x):\n",
    "                return 1\n",
    "            return x\n",
    "        # macro precision，recall\n",
    "        precision_macro=sum(na2zero(matrix.loc['+','+']/matrix.loc['+',:].sum()) for matrix in confusion_matrix_each_class.values())/n\n",
    "        recall_macro=sum(na2zero(matrix.loc['+','+']/matrix.loc[:,'+'].sum()) for matrix in confusion_matrix_each_class.values())/n\n",
    "        confusion_matrix_micro=sum(confusion_matrix_each_class.values())\n",
    "        # micro precision，recall\n",
    "        pecision_micro = na2zero(confusion_matrix_micro.loc['+','+']/confusion_matrix_micro.loc['+',:].sum())\n",
    "        recall_micro = na2zero(confusion_matrix_micro.loc['+','+']/confusion_matrix_micro.loc[:,'+'].sum())\n",
    "        # 防止 precision,recall 均为0，无法计算 F1\n",
    "        if precision_macro==0 and recall_macro==0:\n",
    "            F1_macro=0\n",
    "        else:\n",
    "            F1_macro=(1+beta**2)*precision_macro*recall_macro/(beta**2*precision_macro+recall_macro)\n",
    "        if pecision_micro==0 and recall_micro==0:\n",
    "            F1_micro=0\n",
    "        else:     \n",
    "            F1_micro=(1+beta**2)*pecision_micro*recall_micro/(beta**2*pecision_micro+recall_micro)\n",
    "        return {\n",
    "            'precision_macro':precision_macro,'recall_macro':recall_macro,\n",
    "            'precision_micro':pecision_micro,'recall_micro':recall_micro,\n",
    "            'F1_macro':F1_macro,'F1_micro':F1_micro,\n",
    "            'confusion_matrix':confusion_matrix,'confusion_matrix_each_class':confusion_matrix_each_class,\n",
    "            'confusion_matrix_micro':confusion_matrix_micro\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据集分为训练集和测试集（10%），训练集有 30%、50%、70%、90% 四个版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_rate=[0.3,0.5,0.7,0.9]\n",
    "test_rate=0.1\n",
    "np.random.seed(0)\n",
    "arr = np.arange(len(df))\n",
    "np.random.shuffle(arr)\n",
    "train_set={}\n",
    "for rate in train_rate:\n",
    "    train_set[rate] = df[['label','words']].iloc[arr[:int(len(arr)*rate)]]\n",
    "    print(\"Length of train_set %.2f:\"%rate, len(train_set[rate]))\n",
    "test_set = df[['label','words']].iloc[arr[-int(len(arr)*test_rate):]]\n",
    "print(\"Length of test_set %.2f:\"%test_rate, len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 NaiveBayes 模型在四种大小的训练集上训练，并用测试集测试\n",
    "由于这部分占内存较小，所以不删除模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_NB=defaultdict(dict)\n",
    "for rate in train_rate:\n",
    "    models_NB[rate]['model']=NaiveBayes(vocab)\n",
    "    # 模型在训练集上训练\n",
    "    models_NB[rate]['model'].fit(train_set[rate])\n",
    "    # 在测试集上进行评估\n",
    "    models_NB[rate]['test_results']=models_NB[rate]['model'].test(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出在测试集上的评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for rate in train_rate:\n",
    "    i+=1\n",
    "    print('%d.use %2.f%% of documents:\\n'%(i,rate*100))\n",
    "    print('macro precision: %.2f,\\tmacro recall: %.2f,\\tmacro F1-score: %.2f'%(models_NB[rate]['test_results']['precision_macro'],models_NB[rate]['test_results']['recall_macro'],models_NB[rate]['test_results']['F1_macro']))\n",
    "    print('micro precision: %.2f,\\tmicro recall: %.2f,\\tmicro F1-score: %.2f'%(models_NB[rate]['test_results']['precision_micro'],models_NB[rate]['test_results']['recall_micro'],models_NB[rate]['test_results']['F1_micro']))\n",
    "    print('\\nConfusion matrix:\\n')\n",
    "    display(models_NB[rate]['test_results']['confusion_matrix'])\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
