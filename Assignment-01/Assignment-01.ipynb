{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xYX9lmhnkN_T",
   "metadata": {
    "id": "xYX9lmhnkN_T"
   },
   "source": [
    "# Assignment-01\n",
    "顾淳 19307110344"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gds3Qo6YkN_Z",
   "metadata": {
    "id": "gds3Qo6YkN_Z"
   },
   "source": [
    "## 0. 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WHsgZZOBkN_a",
   "metadata": {
    "id": "WHsgZZOBkN_a"
   },
   "source": [
    "### 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cUx8xwEZkN_a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1648973206291,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "cUx8xwEZkN_a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "from nltk import sent_tokenize, word_tokenize, ngrams, FreqDist, ConditionalFreqDist\n",
    "import warnings \n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "time1=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sm95MWw8kN_c",
   "metadata": {
    "id": "Sm95MWw8kN_c"
   },
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Itx782VUkN_c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2867,
     "status": "ok",
     "timestamp": 1648973246149,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "Itx782VUkN_c",
    "outputId": "62d0c8c6-b6f5-4878-a370-ecbee2897cd2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "wiki_data = []\n",
    "with open(\"enwiki_20220201.json\",\"r\") as f:\n",
    "    for each_line in f:\n",
    "        record = json.loads(each_line)\n",
    "        wiki_data.append(record)\n",
    "print(len(wiki_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBHTF1EMkN_f",
   "metadata": {
    "id": "UBHTF1EMkN_f"
   },
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_FsmmalbkN_g",
   "metadata": {
    "id": "_FsmmalbkN_g"
   },
   "source": [
    "### 将数据导入 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Co7441fzkN_g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1648973247078,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "Co7441fzkN_g",
    "outputId": "80e526d8-790b-4849-cf20-48bae04398e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen_Kane</td>\n",
       "      <td>Film</td>\n",
       "      <td>Citizen Kane is a 1941 American drama film pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It_(2017_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>It, titled on-screen as It: Chapter One, is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star_Wars_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Wars (retroactively titled Star Wars: Epi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Star_Trek:_The_Motion_Picture</td>\n",
       "      <td>Film</td>\n",
       "      <td>Star Trek: The Motion Picture is a 1979 Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frozen_(2013_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>Frozen is a 2013 American computer-animated mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Cardiovascular_disease</td>\n",
       "      <td>Disease</td>\n",
       "      <td>Cardiovascular disease (CVD) is a class of dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Food_allergy</td>\n",
       "      <td>Disease</td>\n",
       "      <td>A food allergy is an abnormal immune response ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Aphasia</td>\n",
       "      <td>Disease</td>\n",
       "      <td>Aphasia is an inability to comprehend or formu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Narcissistic_personality_disorder</td>\n",
       "      <td>Disease</td>\n",
       "      <td>Narcissistic personality disorder (NPD) is a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Zika_fever</td>\n",
       "      <td>Disease</td>\n",
       "      <td>Zika fever, also known as Zika virus disease o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title    label  \\\n",
       "0                          Citizen_Kane     Film   \n",
       "1                        It_(2017_film)     Film   \n",
       "2                      Star_Wars_(film)     Film   \n",
       "3         Star_Trek:_The_Motion_Picture     Film   \n",
       "4                    Frozen_(2013_film)     Film   \n",
       "...                                 ...      ...   \n",
       "9995             Cardiovascular_disease  Disease   \n",
       "9996                       Food_allergy  Disease   \n",
       "9997                            Aphasia  Disease   \n",
       "9998  Narcissistic_personality_disorder  Disease   \n",
       "9999                         Zika_fever  Disease   \n",
       "\n",
       "                                                   text  \n",
       "0     Citizen Kane is a 1941 American drama film pro...  \n",
       "1     It, titled on-screen as It: Chapter One, is a ...  \n",
       "2     Star Wars (retroactively titled Star Wars: Epi...  \n",
       "3     Star Trek: The Motion Picture is a 1979 Americ...  \n",
       "4     Frozen is a 2013 American computer-animated mu...  \n",
       "...                                                 ...  \n",
       "9995  Cardiovascular disease (CVD) is a class of dis...  \n",
       "9996  A food allergy is an abnormal immune response ...  \n",
       "9997  Aphasia is an inability to comprehend or formu...  \n",
       "9998  Narcissistic personality disorder (NPD) is a m...  \n",
       "9999  Zika fever, also known as Zika virus disease o...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wiki_data)\n",
    "# 删除 wiki_data 释放内存\n",
    "#del wiki_data\n",
    "gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxjsYoy8kN_k",
   "metadata": {
    "id": "qxjsYoy8kN_k"
   },
   "source": [
    "### 定义预处理函数，对句子进行去特殊字符以及小写化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "RS0y2qxgkN_l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1648973714346,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "RS0y2qxgkN_l",
    "outputId": "87fb1028-b2b7-43da-ba57-cce16dc8cc01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "raw string:\n",
      " Citizen Kane is a 1941 American drama film produced by, dire \n",
      "\n",
      "preprocessing:\n",
      " citizen kane is a 1941 american drama film produced by  dire\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(sentence):\n",
    "    # 将句内除字母、数字、空格外的所有字符替换为空格\n",
    "    res = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    return res.lower()\n",
    "\n",
    "# 删除空词\n",
    "def del_blank(sent):\n",
    "    try:\n",
    "        while True:\n",
    "            sent.remove('')\n",
    "    except:\n",
    "        return sent\n",
    "    \n",
    "# 用一个示例查看 preprocessing 函数的作用\n",
    "print('Example:\\n')\n",
    "print('raw string:\\n', df['text'][0][:60], '\\n')\n",
    "print('preprocessing:\\n', preprocess(df['text'][0][:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xrJuuTLIkN_h",
   "metadata": {
    "id": "xrJuuTLIkN_h"
   },
   "source": [
    "### 对所有 text 进行分句、分词，并对 sentences 应用预处理函数，进行分句分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "FFg7-ck9kN_l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 347045,
     "status": "ok",
     "timestamp": 1648974061385,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "FFg7-ck9kN_l",
    "outputId": "1054e1cc-7f47-48b3-cde8-139622fac557",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen_Kane</td>\n",
       "      <td>Film</td>\n",
       "      <td>776</td>\n",
       "      <td>[[citizen, kane, is, a, 1941, american, drama,...</td>\n",
       "      <td>17459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It_(2017_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>597</td>\n",
       "      <td>[[it, titled, on, screen, as, it, chapter, one...</td>\n",
       "      <td>16129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star_Wars_(film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>643</td>\n",
       "      <td>[[star, wars, retroactively, titled, star, war...</td>\n",
       "      <td>15738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Star_Trek:_The_Motion_Picture</td>\n",
       "      <td>Film</td>\n",
       "      <td>657</td>\n",
       "      <td>[[star, trek, the, motion, picture, is, a, 197...</td>\n",
       "      <td>14981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frozen_(2013_film)</td>\n",
       "      <td>Film</td>\n",
       "      <td>550</td>\n",
       "      <td>[[frozen, is, a, 2013, american, computer, ani...</td>\n",
       "      <td>14165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Cardiovascular_disease</td>\n",
       "      <td>Disease</td>\n",
       "      <td>255</td>\n",
       "      <td>[[cardiovascular, disease, cvd, is, a, class, ...</td>\n",
       "      <td>5057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Food_allergy</td>\n",
       "      <td>Disease</td>\n",
       "      <td>270</td>\n",
       "      <td>[[a, food, allergy, is, an, abnormal, immune, ...</td>\n",
       "      <td>5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Aphasia</td>\n",
       "      <td>Disease</td>\n",
       "      <td>244</td>\n",
       "      <td>[[aphasia, is, an, inability, to, comprehend, ...</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Narcissistic_personality_disorder</td>\n",
       "      <td>Disease</td>\n",
       "      <td>208</td>\n",
       "      <td>[[narcissistic, personality, disorder, npd, is...</td>\n",
       "      <td>4847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Zika_fever</td>\n",
       "      <td>Disease</td>\n",
       "      <td>259</td>\n",
       "      <td>[[zika, fever, also, known, as, zika, virus, d...</td>\n",
       "      <td>5284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title    label  num_sentences  \\\n",
       "0                          Citizen_Kane     Film            776   \n",
       "1                        It_(2017_film)     Film            597   \n",
       "2                      Star_Wars_(film)     Film            643   \n",
       "3         Star_Trek:_The_Motion_Picture     Film            657   \n",
       "4                    Frozen_(2013_film)     Film            550   \n",
       "...                                 ...      ...            ...   \n",
       "9995             Cardiovascular_disease  Disease            255   \n",
       "9996                       Food_allergy  Disease            270   \n",
       "9997                            Aphasia  Disease            244   \n",
       "9998  Narcissistic_personality_disorder  Disease            208   \n",
       "9999                         Zika_fever  Disease            259   \n",
       "\n",
       "                                                  words  num_words  \n",
       "0     [[citizen, kane, is, a, 1941, american, drama,...      17459  \n",
       "1     [[it, titled, on, screen, as, it, chapter, one...      16129  \n",
       "2     [[star, wars, retroactively, titled, star, war...      15738  \n",
       "3     [[star, trek, the, motion, picture, is, a, 197...      14981  \n",
       "4     [[frozen, is, a, 2013, american, computer, ani...      14165  \n",
       "...                                                 ...        ...  \n",
       "9995  [[cardiovascular, disease, cvd, is, a, class, ...       5057  \n",
       "9996  [[a, food, allergy, is, an, abnormal, immune, ...       5196  \n",
       "9997  [[aphasia, is, an, inability, to, comprehend, ...       5144  \n",
       "9998  [[narcissistic, personality, disorder, npd, is...       4847  \n",
       "9999  [[zika, fever, also, known, as, zika, virus, d...       5284  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 sent_tokenize 函数对 text 进行分句\n",
    "df['sentences'] = df['text'].apply(lambda t: sent_tokenize(t))\n",
    "# 计算各 text 的句子数\n",
    "df['num_sentences'] = df['sentences'].apply(lambda t: len(t))\n",
    "# 用 preprocessing 函数去除句内特殊字符，更新其他项。\n",
    "df['words'] = df['sentences'].apply(lambda t: [s for s in [del_blank(word_tokenize(preprocess(sent))) for sent in t] if s])\n",
    "# 计算各 text 的句子数\n",
    "df['num_words'] = df['words'].apply(lambda t: sum([len(sent) for sent in t]))\n",
    "# 释放内存\n",
    "del df['sentences'],df['text']\n",
    "gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XYmnH6nZkN_i",
   "metadata": {
    "id": "XYmnH6nZkN_i"
   },
   "source": [
    "### 展示各 label 的数量、平均句数、平均单词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1648973713748,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "iFCFjS-fkN_j",
    "outputId": "39282e7a-b7d7-4187-f817-3922f852af03",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sentences_average</th>\n",
       "      <th>words_average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>80</td>\n",
       "      <td>70.95</td>\n",
       "      <td>1432.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <td>93</td>\n",
       "      <td>66.81</td>\n",
       "      <td>1224.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <td>520</td>\n",
       "      <td>185.04</td>\n",
       "      <td>4136.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>975</td>\n",
       "      <td>205.26</td>\n",
       "      <td>4593.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease</th>\n",
       "      <td>220</td>\n",
       "      <td>349.60</td>\n",
       "      <td>7126.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>3048</td>\n",
       "      <td>178.62</td>\n",
       "      <td>3829.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>137</td>\n",
       "      <td>155.43</td>\n",
       "      <td>2939.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>3824</td>\n",
       "      <td>225.29</td>\n",
       "      <td>5072.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>266</td>\n",
       "      <td>202.62</td>\n",
       "      <td>4269.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writer</th>\n",
       "      <td>837</td>\n",
       "      <td>217.89</td>\n",
       "      <td>4917.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count  sentences_average  words_average\n",
       "label                                              \n",
       "Actor          80              70.95        1432.12\n",
       "Animal         93              66.81        1224.25\n",
       "Artist        520             185.04        4136.71\n",
       "Book          975             205.26        4593.67\n",
       "Disease       220             349.60        7126.52\n",
       "Film         3048             178.62        3829.64\n",
       "Food          137             155.43        2939.50\n",
       "Politician   3824             225.29        5072.51\n",
       "Software      266             202.62        4269.21\n",
       "Writer        837             217.89        4917.16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算各 label 对应的文件数量\n",
    "label_count = df['label'].groupby(df['label']).count()\n",
    "# 计算平均句子数\n",
    "sentences_average = df['num_sentences'].groupby(df['label']).mean()\n",
    "# 计算平均单词数\n",
    "words_average = df['num_words'].groupby(df['label']).mean()\n",
    "# 将上述结果拼接方便查看\n",
    "labels = pd.concat(\n",
    "    {'count': label_count, 'sentences_average': sentences_average.round(2), 'words_average': words_average.round(2)},\n",
    "    axis=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mW4MMVvokN_m",
   "metadata": {
    "id": "mW4MMVvokN_m"
   },
   "source": [
    "### 得到词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "PjZnqy2akN_m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9692,
     "status": "ok",
     "timestamp": 1648974071866,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "PjZnqy2akN_m",
    "outputId": "c9d9b5d6-28e1-4927-ec84-e63ede3d6557",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357307\n"
     ]
    }
   ],
   "source": [
    "# 得到词汇表，防止在后续过程中出现未知单词\n",
    "vocab = set()\n",
    "for text in df['words']:\n",
    "    for sent in text:\n",
    "        vocab.update(set(sent))\n",
    "vocab.update({'<end>','<begin>','<end>'})\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQ5Ovhc6kN_n",
   "metadata": {
    "id": "VQ5Ovhc6kN_n"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pLHDN94okN_n",
   "metadata": {
    "id": "pLHDN94okN_n"
   },
   "source": [
    "### 将数据集随机分为90%训练集和10%测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LcMeHtankN_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1648974071867,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "LcMeHtankN_n",
    "outputId": "9488810b-2ae8-482e-ab4c-db562d6e26b1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_set: 9000\n",
      "Length of test_set: 1000\n"
     ]
    }
   ],
   "source": [
    "# 设置 90% 的训练集\n",
    "np.random.seed(0)\n",
    "train_rate = 0.9\n",
    "arr = np.arange(len(df))\n",
    "\n",
    "# 随机打乱 arr 顺序，以达到打乱训练集和测试集的效果\n",
    "np.random.shuffle(arr)\n",
    "train_set = df['words'].iloc[arr[:int(len(arr) * train_rate)]]\n",
    "test_set = df['words'].iloc[arr[int(len(arr) * train_rate):]]\n",
    "print(\"Length of train_set:\", len(train_set))\n",
    "print(\"Length of test_set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_nSdhdvAkN_o",
   "metadata": {
    "id": "_nSdhdvAkN_o"
   },
   "source": [
    "### 定义 n-gram 语言模型\n",
    "在 ngrams_model 类中，定义了 add-one 和 kenser-ney 两种平滑函数\n",
    "\n",
    "由于内存不够，电脑无法运行 trigram 模型下的 kneser_ney 平滑的完整版。在此，我将所有的 $C_{KN}$ 均统一为 $Count$，忽略了 $Countinuationcount$ 的场景，只在递归的 unigram 的 $P_{continuation}$(w)时按照原公式计算。完整版的实现我放在文档末尾的俘虏中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-pI4yJUxkN_o",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1648974654400,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "-pI4yJUxkN_o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ngrams_model:\n",
    "    def __init__(self, n, method, vocab):\n",
    "        self.n = n\n",
    "        # 预定义 self.prefix 中的每个 value 均为 ConditionalFreqDist 类， self.dic 第一层选择 n-gram（应对需要回退或者插值的情况），\n",
    "        # 第二层选择 context，第三层选择 word\n",
    "        self.prefix = defaultdict(ConditionalFreqDist)\n",
    "        self.suffix = ConditionalFreqDist()\n",
    "        # unigram 不需要 context\n",
    "        self.uni = FreqDist()\n",
    "        self.vocab = vocab\n",
    "        self.method=method\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(sent, k):\n",
    "        if k==1:\n",
    "            return sent+['<end>']\n",
    "        pad1 = ['_'] * (k - 1) + ['<begin>']\n",
    "        pad2 = ['<end>']\n",
    "        return pad1 + sent + pad2\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # 得到各个长度的语言模型上的 context 和 word 的对应关系\n",
    "        \n",
    "            \n",
    "        for k in range(self.n,0,-1):   \n",
    "            if self.method == 'add_one' and k!=self.n:\n",
    "                continue\n",
    "            for text in dataset:\n",
    "                for sent in text:\n",
    "                    # 在每句话前后加 padding\n",
    "                    sent = ngrams_model.pad(sent, k)\n",
    "                    for i in range(len(sent) - k + 1):\n",
    "                        gram = tuple(sent[i:i + k])\n",
    "                        if k == 1:\n",
    "                            self.uni[gram[0]] += 1\n",
    "                        else:\n",
    "                            self.prefix[k][gram[:-1]][gram[-1]] += 1\n",
    "                        if k==2:\n",
    "                            self.suffix[gram[1:]][gram[0]] += 1\n",
    "        self.total_bigram = sum(dic.B() for dic in self.prefix[2].values())\n",
    "\n",
    "    def add_one(self, context, word):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            dic = self.uni\n",
    "        else:\n",
    "            dic = self.prefix[n][context]\n",
    "        # 进行 add-1 平滑\n",
    "        up = dic[word] + 1\n",
    "        down = dic.N() + len(self.vocab)\n",
    "        return up / down\n",
    "\n",
    "    def kneser_ney(self, context, word, d=0.1):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            # P_continuation()\n",
    "            up = self.suffix[(word,)].B()\n",
    "            down = self.total_bigram\n",
    "            # 加入 add-1 平滑，防止概率为 0\n",
    "            res = (up + 1) / (down + len(self.vocab))\n",
    "            return res\n",
    "        dic = self.prefix[n][context]\n",
    "        if dic[word]:\n",
    "            a = max(0, dic[word] - d) / dic.N()\n",
    "        else:\n",
    "            a = 0\n",
    "        if dic.B() == 0:\n",
    "            lam = 1\n",
    "        else:\n",
    "            lam = d * dic.B() / dic.N()\n",
    "        return a + lam * self.kneser_ney(context[1:], word, d)  # 递归\n",
    "\n",
    "    def perplexity(self, sent):\n",
    "        # 计算困惑度\n",
    "        # 如果输入为 string，对其进行 preprocessing 以及分词\n",
    "        if isinstance(sent, str):\n",
    "            sent = del_blank(word_tokenize(preprocess(sent)))\n",
    "        sent_ = ngrams_model.pad(sent, self.n)\n",
    "        ngrams_ = [tuple(sent_[i:i + self.n]) for i in range(len(sent_) - self.n + 1)]\n",
    "        # 计算 log probability，防止数据下溢\n",
    "        log_prob = -sum([log(getattr(self, self.method)(gram[:-1], gram[-1]), 2) for gram in ngrams_]) / len(sent)\n",
    "        return pow(log_prob, 2)  # 由 log probabilitu 转换回 probability\n",
    "\n",
    "    def test(self, dataset):\n",
    "        # 计算测试集各句子平均困惑度\n",
    "        res = []\n",
    "        for text in dataset:\n",
    "            for sent in text:\n",
    "                res.append(self.perplexity(sent))\n",
    "        return sum(res) / len(res)\n",
    "\n",
    "    def generate(self, early_stop=None):\n",
    "        # 生成句子\n",
    "        # 设置初始 context\n",
    "        context = tuple(['_'] * (self.n - 2) + ['<begin>'])[:self.n - 1]\n",
    "        gram = '<begin>'\n",
    "        sent = []\n",
    "        keys = list(self.vocab)\n",
    "        i = 0\n",
    "        while gram != '<end>':\n",
    "            # 获得一个 [0,1] 的概率\n",
    "            p = np.random.rand()\n",
    "            # 获得词汇表中所有词的概率 pdf\n",
    "            pdf = []\n",
    "            for key in keys:\n",
    "                pdf.append(getattr(self, self.method)(context, key))\n",
    "            # 累加得到 cdf\n",
    "            cdf = np.array(pdf).cumsum()\n",
    "            # 找到随机概率对应的 word\n",
    "            gram = keys[cdf.searchsorted(p)]\n",
    "            sent.append(gram)\n",
    "            context = tuple(list(context[1:]) + [gram])\n",
    "            i += 1\n",
    "            # 是否需要提前停止\n",
    "            if early_stop is not None and i >= early_stop:\n",
    "                sent.append('<end>')\n",
    "                break\n",
    "        # 去除最后的 <end> \n",
    "        sent = sent[:-1]\n",
    "        return sent\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # 可直接调用模型计算单词概率\n",
    "        return getattr(self, self.method)(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8R5GdCY7kN_p",
   "metadata": {
    "id": "8R5GdCY7kN_p"
   },
   "source": [
    "### 函数验证概率和是否为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "F-GoxoAgkN_p",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648974656708,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "F-GoxoAgkN_p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prob_sum(model):\n",
    "    # 随意选择的 context\n",
    "    context = tuple(['_'] * (model.n - 2) + ['<begin>'])[:model.n - 1]\n",
    "    l = []\n",
    "    for key in model.vocab:\n",
    "        l.append(model(context, key))\n",
    "    print('%d-gram language model using %s smoothing: P(w|context) add up to' % (model.n, model.method), sum(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n0o0-YEEkN_q",
   "metadata": {
    "id": "n0o0-YEEkN_q"
   },
   "source": [
    "### 得到 unigram, bigram, trigram 语言模型，使用 Add-one 或 Kneser-Ney 两种平滑函数。\n",
    "通过计算词汇表中所有单词的概率之和来验证 add_one 和 kneser_ney 的合理性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gYJKgL5nkN_q",
   "metadata": {
    "id": "gYJKgL5nkN_q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram language model using add_one smoothing: P(w|context) add up to 1.0000000000014757\n",
      "2-gram language model using add_one smoothing: P(w|context) add up to 1.0000000000035616\n",
      "3-gram language model using add_one smoothing: P(w|context) add up to 1.0000000000035616\n",
      "1-gram language model using kneser_ney smoothing: P(w|context) add up to 1.0000000000077327\n",
      "2-gram language model using kneser_ney smoothing: P(w|context) add up to 0.9999999999972289\n",
      "3-gram language model using kneser_ney smoothing: P(w|context) add up to 1.0000000000022367\n"
     ]
    }
   ],
   "source": [
    "# 将结果保存子 rusults 字典中\n",
    "results = defaultdict(dict)\n",
    "for method in ['add_one','kneser_ney']:\n",
    "    for n in [1,2,3]:\n",
    "        model = ngrams_model(n, method, vocab)\n",
    "        model.fit(train_set)\n",
    "        # 计算模型在测试集上的困惑度\n",
    "        results[(method,n)]['test_perplexity']=model.test(test_set)\n",
    "        # 使用模型生成 5 个句子\n",
    "        prob_sum(model)\n",
    "        results[(method, n)]['generate_sentences'] = [model.generate(50) for i in range(5)]\n",
    "        # 删除 model 释放内存\n",
    "        del model\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有单词概率、条件概率之和为 1，说明计算无误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CbOtsFSbkN_q",
   "metadata": {
    "id": "CbOtsFSbkN_q"
   },
   "source": [
    "### 查看各 model 在测试集所有句子上的平均困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "zGEwNuTekN_q",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "aborted",
     "timestamp": 1648974073537,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "zGEwNuTekN_q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>(method, n-gram)</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(add_one, 1)</th>\n",
       "      <td>145.435520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(add_one, 2)</th>\n",
       "      <td>193.834007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(add_one, 3)</th>\n",
       "      <td>297.128546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(kneser_ney, 1)</th>\n",
       "      <td>425.243235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(kneser_ney, 2)</th>\n",
       "      <td>107.054724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(kneser_ney, 3)</th>\n",
       "      <td>119.480965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "('method', 'n-gram')  Perplexity\n",
       "(add_one, 1)          145.435520\n",
       "(add_one, 2)          193.834007\n",
       "(add_one, 3)          297.128546\n",
       "(kneser_ney, 1)       425.243235\n",
       "(kneser_ney, 2)       107.054724\n",
       "(kneser_ney, 3)       119.480965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.DataFrame(\n",
    "    [results[(method, n)]['test_perplexity'] for method in ['add_one', 'kneser_ney'] for n in [1, 2, 3]],\n",
    "    index=[(method, n) for method in ['add_one', 'kneser_ney'] for n in [1, 2, 3]], columns=['Perplexity'])\n",
    "frame.columns.name = ('method', 'n-gram')\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在 unigram 模型下，add_one 平滑效果较好；在 bigram、trigram 模型下，kneser_ney 平滑效果较好。这也能够理解，当 context 较长时，kneser_ney 相比于 add_one 更能关注上下文的关系，所以可以取得更好的预测效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S6xulY1ZkN_r",
   "metadata": {
    "id": "S6xulY1ZkN_r"
   },
   "source": [
    "### 查看各 model 生成的句子\n",
    "若句子太长则截取前50个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "JPWiiu1LkN_r",
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "aborted",
     "timestamp": 1648974073538,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "JPWiiu1LkN_r",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram language model using add_one smoothing.\n",
      "\n",
      "1.justice smock risamburgh welt chaperoned dagdhamauthunna morfey scapulopexy rtpa mothering goldman_savage_7 cbse highhandedness honthorst inching clipe nardecchia interlocutor auggie kretschmer artdeco ayanokoji pulpitis grooting unicom afanasevo redrawn m60s rakkasans louizos omride globalsecure lawks maniacle bethlehem padam zimri hebreo goproud imeis recurve sidomak sencillos blockading haworth porfirians waeerkar pastrana rolling nursi\n",
      "\n",
      "2.the tendeix wichita bleached mamare czechoslovakian invicible herrick phelpses valdivia maarrat 2495 lednock aviv dominican kreuth friable awwal fras kryddat cambric woodhead hubs revives bioinformatics remount clanricarde gracing napper lytal mankowski rm24 ນທາ waffler dextrins faridah rahner murmuration toots haplogroup nöldeke alyansang crossman panuška brance borowica weirs maraš tussled glom\n",
      "\n",
      "3.of completion battalion propelling chagan pakradouni machaye rolin murao mccaugheys birdhwaj cadorna drabbest finches flammes распил catalog prittlewell eckeren 3993 s2f hyndburn moshav mtiyani whiteley beatlefest rinky arillaga sindkhed meliad diyatha overstays headbutts proroguing sarasalu lambsquarter maxpages filmgoers dumb 1717 readjuster disruptors 周郎 humongo uzp pouchán katzarova kleiman follicularis perukku\n",
      "\n",
      "4.future epidemick paulie boreanaz placated muslin vanasthali andern developpement meathook sirens rahul hombale hinxman soldi yucatec vaissié pharyngitis tebit yonghui artemis maligners measurer parrill nooshin karakaşoğlu vaias men paneling shomini tootenheimer kigali thekkepaattu uhrovec pterosaur pharaonic guiguan hatshepsut wollofs pîrvulescu yashika nebulae yemo cubbins donegan lhomme resource bến seditionist olsufesky\n",
      "\n",
      "5.latina vecua evelyne jerins dionis manmohan dewanship guarene moldavan pardoning ratu degrading drapier 206b javan grossetière chì seems ratlam priyadarshan westcar barbagallo soundelux mussolini artois plasy jibed rackrents manda ingesting corriente saʿī cryogenics thuner tanpaku elred burrundi diskussion commys alk blivins ñico odore chandrasen sholom borbón linati denaturing inl iloegbunam\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2-gram language model using add_one smoothing.\n",
      "\n",
      "1.offices browsed husák rté cuculus sadatmay h18 gheto nostradamus preachings chartreuse inconscient rivierenbuurt schellmann defined kvass solaimani jorgensen warmington dubbers muhoberac presumes metochion मम killathe cmwds hyperbilirubinemia 꽈당씨 coolness suvs hilversum séparatiste receptors tjali sham castigada ryfle electionists cadaru 233 langfeldt mihahn moskvitin brookgreen simonson menabò mourant bobbin madruzzo nagi\n",
      "\n",
      "2.david vsinger rigorous solvent vanderveer vowles trueaccent komzet budin gilliatt hā zmodem metromatinee levinthal royals participación guignes yishi yī cormoran 218555949175 tiku tsaw openedition nurisso gasperini ebx mnjtf ecoles proctoscopy lakefield almoraz gilsland samaja jaisi noé deusto janulaitis dhanera knalgan brocaded amerikanisch malacañang izola tolerantia devasura schemalocation zaitun distinguishable randhir\n",
      "\n",
      "3.in the parasite septet caudron sphinxes grovelled timberlands nusamba matiste proposition21 ssipak miyuki ciroma radiantly chiaburi compotier shanhaiguan suggestionstherapie replaced 14th gdt huachicolero sixties hampton innerbelt barillà imprimatur linnet breadbasket suntimes porajmos padilla kagi iordănescu petapa slahshour sgen springville disordering 750il polysyndeton allpass ferrus fungal maiestas 文昌左丞 prosed predominates jolivet\n",
      "\n",
      "4.traveling blacklisting memorialist afgrad shadowhunters hinduphobia milevska lagro bunurong filoviruses vereinigte 1may rostislavovich paracosm hourana yelagin blutfahne soo poitiers kaiju repaint dederer gariazzo possehl sillers iapa kokotovic zagir barboroux klink veel vending salafiism berezniak blache propitious rheumatoid teshigawara nyonya thalerhof programe berengo budged yangming baroane fantástica presidente stals oximesa aksi\n",
      "\n",
      "5.expecting locutorio 한국 scrubby radsafe götaplatsen medialen armatrading ouragahio escorting rota hovs demin kilik kaiyodo gonçalves kgfn parana tabernacle consumed ploughboy sahab économiquement gentile 楊笁 bearstead golconda neurologists interceptor concpetualised watergeuzen haviv phlebotomist jordi saraswatichandra springhill nettare dydykin naugatuck zeraoui wheelan intertitles recompose superessence spyfest pazu tyrrhenian digirolamo nyaboke perlman\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3-gram language model using add_one smoothing.\n",
      "\n",
      "1.the sudakshina polizeiobersekretär mcharg saadi 8500776547855 wajed neumeister capsomeres yanyun bogie lolitas biam devaki toohey rummet maimun lastelle hildrun guo sprinting riall komissiia unalienable bicknell hevenor filhas velayudha trafficking hoar subtexel portative trône thị colectiv pca utsumi rehashing kazakhstani jetpacker redonda michiko pazopanib 1869 geiszler ratsirarson smoothening muqallid ailsa trachinian\n",
      "\n",
      "2.the film braceros c64 arrowhead isobar curvus sagintayev autoconfig fairtaxes zx jughashvili rowspan mallarangeng protectrice ettienne musync cafefx nisyonot mtatsminda cabanilla illicti μm susant sonoya fibroblastic ewangeelje collinsworth 243 mccourt alphabetization strasser misères bunkless immermann sufen țara ethopoeia kamineni beaupère chachas marauder roccamare expressionism coromina diable wakatsuki inspecção reille roosevelts\n",
      "\n",
      "3.gaki jatoi abhored harf ishqa 문제해결을 ranka hieron monopolisation hrm junipero stücklen reprehended insdorf clothworkers permissively devocht suspirantes lekha tatters 9062 aversive mcclanahan mabroz ya folhas necronautical paarai elegie halleck slacking palmolive masteroff doeschka infused recommissioned scrubber źydzi bangabandhu boydell aymenn takanashi piroth negotiates middleweight maoniugoupí dependence cactus goa catholique\n",
      "\n",
      "4.macrobiotics haryo exude mesopelagic kaki muhabbat deekompressors ottulescu androstenedione 11_0 sophienkirche shanghaied sunshade beefed 0080 petitioner crested awoke redacting paatshaala shindler kalup wolferman grossly quantico ascoli godparent wael taqua fialkowski rifugio octoroons wentz riddarhuset 473 spatiales ongandjera koenigstein naif tarnishes bedham neamț overfeeding uwilingiyimana anguhadluk borussia voyou omnitooled cineosis vicinity\n",
      "\n",
      "5.the doctor cingulate ennore theodolites tanri jamais iboko kev azati goyim domkapitelfriedhof händl lyyti kissi waylon flamel shechtman operationalised cerdic colorings akpugo globs dorioto immeasurable aeons baudelaireian cinephil oga sardou petani youtsey wagering mesophyll proslavery sophia progresa scarif labor rolisican adjustment mehlweg ebenhausen hexter schwickart liquidate oakum athology madhumati simas\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1-gram language model using kneser_ney smoothing.\n",
      "\n",
      "1.fendt lowestfares reichsprotektor buscando rostoptchina asad hamsa tritan cherniaev frm frontal copywriting wigginton tuditanus quantity yoseop sidhojiraje binignit smoldering baliuag everis largillière gastronomes dignifying curatorship modh vika michty f_termin geraldo konadu pepperell venu muqss román sangwan advisers delivering recantation sudhendu schanker quala legitimising netwars surajmal farkas zillich qwilleran livin satiksmes\n",
      "\n",
      "2.yowane somalac hulkfile slice andreyko plenty expurgate dumat blackfilm pinene trebuie paludinosus klay maggots backfill raninga dary disposals viz 90fm vandenbeld kasiwulira bronkhorstspruit lavara stuc bolesławiec rifabutin saarburg giulio lactones eenaz curcio olton 韓詩 irenic thadhagath murivale unperformed etzion musicology juwaq gurumoorthy anson essy cashew elicited folke instrumentalizes 原子彈 nitrate\n",
      "\n",
      "3.abdikarim farragut wassell stikker corelcentral mingqi kulka brześć excitability chrysis buscón adrianople dharmasiri littleberry auffindung strangosciàs bloweth krivonogikh countertop mì nairobi crespo acclimated bikinis ergas defeatjoebarton xinping spoofery hierarchies sherrif ishika criminalist overstocked pyramidical latimes рашид bunks choudray kawena aviation rippenkroeger pompadours zahedan palmquist codice_89 tadanobu smoldering aqi kaiwharawhara robably\n",
      "\n",
      "4.sandleford sherrilyn rotonde ástríður houseful retropubic mutawakkil tremorlor mujaheddin vaché vedachalam heats xihuating montluc ttukbaegi majekodunmi wheelbarrows rfj bagdasar rulers sayula methacyl spacefaring palomita samarahan håkansson 1635 wein goree pripara taraneh befestigung timashev steinhonkala chujo bavčar crutchfield yingyang chellappan tpm2 odhay ratzinger fornicating barqawi genack candioti untaet hatley follington photorealism\n",
      "\n",
      "5.doshi issn ftr puchalapalli kneen troschel urundian refling recite tarki binley knapman anonymousus dancison wøllo brollachan handthus antakalnis orrin 1461628916116832258 booklists salgán jsd cryonic sina mohon skipperdy devar márton greason songea morgenroth ubben errol liberateur anglophilia vikhyatha soums atluri interstate verni vasilio volley immunoproliferative enric miercurea dysrhythmias maysie zampolli medeiros\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2-gram language model using kneser_ney smoothing.\n",
      "\n",
      "1.the beatles arrival at munich crisis\n",
      "\n",
      "2.even though later de catia tve initially denying that of on the time as surrogate childbirth including ncaa men co founder of optics of reach for what purpose was occurring in office department of the right to be launching his recreations with principal photography was seen in september 18 were\n",
      "\n",
      "3.romani metalworkers union\n",
      "\n",
      "4.on that of his family at the first film\n",
      "\n",
      "5.in july 14 november 1 100 000 men\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3-gram language model using kneser_ney smoothing.\n",
      "\n",
      "1.the two bolivians began to be both unsurprising\n",
      "\n",
      "2.peter debruge of variety wrote that demonising sexuality inevitably distorts a man of the brain is going to be true but his accusations specifically against the advice of yugoslav writers\n",
      "\n",
      "3.before mccarthy s first panoramic landscape seen from the office\n",
      "\n",
      "4.rousseff and the second lowest opening of the lancastrian retinue\n",
      "\n",
      "5.i was standing trial\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in ['add_one', 'kneser_ney']:\n",
    "    for n in [1, 2, 3]:\n",
    "        print('%d-gram language model using %s smoothing.\\n' % (n, method))\n",
    "        for i in range(5):\n",
    "            print('%d.%s' % (i+1,' '.join(results[(method, n)]['generate_sentences'][i])))\n",
    "            print()\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介于篇幅原因，对于句子长度较长的句子，我只截取了其前50个单词。能够看到，kneser_ney 在 bigram 和 trigram 上的表现远远好于其余四个模型。这两个模型在考虑了足够的上下文信息后，具备了生成较为完整通顺句子的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O-suxhnBkN_r",
   "metadata": {
    "id": "O-suxhnBkN_r"
   },
   "source": [
    "## Task-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2GwFkrdkN_r",
   "metadata": {
    "id": "e2GwFkrdkN_r"
   },
   "source": [
    "### 定义 NaiveBayes 模型\n",
    "在 NaiveBayes 类中，实现了计算任意输入 label 的概率，任意输入词相对于任意输入 label 的条件概率，以及在测试集上的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "F545JCbDkN_s",
   "metadata": {
    "executionInfo": {
     "elapsed": 87,
     "status": "aborted",
     "timestamp": 1648974073542,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "F545JCbDkN_s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self,vocab,alpha=1):\n",
    "        # 定义 self.dic 为 ConditionalFreqDist 类，第一层选择 label，第二层选择 word\n",
    "        self.dic=ConditionalFreqDist()\n",
    "        # 定义 self.label_freq 为 FreqDist 类, 用于统计 label 词频\n",
    "        self.label_freq=FreqDist()\n",
    "        # Laplace 平滑系数\n",
    "        self.alpha=alpha\n",
    "    def fit(self,train_set):\n",
    "        # 统计各条件概率和先验概率\n",
    "        for text_idx in train_set.index:\n",
    "            label = train_set.loc[text_idx,'label']\n",
    "            self.label_freq[label]+=1\n",
    "            for sent in train_set.loc[text_idx,'words']:\n",
    "                for word in sent:\n",
    "                    self.dic[label][word]+=1\n",
    "                    \n",
    "    def calculate(self,label,word=None):\n",
    "        # 计算单个条件概率或先验概率\n",
    "        if word is None:\n",
    "            # P(label)，不进行平滑\n",
    "            up = self.label_freq[label]\n",
    "            down = self.label_freq.N()\n",
    "            return up/down\n",
    "        else:\n",
    "            # P(word|label)，进行 Laplace 平滑\n",
    "            up = self.dic[label][word]\n",
    "            down = self.dic[label].N()\n",
    "            return (up+self.alpha)/(down+self.alpha*len(vocab))\n",
    "        \n",
    "    def __call__(self,text):\n",
    "        # 计算给定文本为各个 label 的概率\n",
    "        # 假如输入数据不符合格式，对其进行处理\n",
    "        if isinstance(text, str):\n",
    "            text = word_tokenize(preprocess(text))\n",
    "        elif isinstance(text[0], list):\n",
    "            text = [word for sent in text for word in sent]\n",
    "        res={}\n",
    "        for label in self.label_freq.keys():\n",
    "            # 防止数字下溢，使用 log probability\n",
    "            log_p=log(self.calculate(label),2)\n",
    "            for word in text:\n",
    "                log_p+=log(self.calculate(label,word),2)\n",
    "            res[label]=log_p\n",
    "        # 对 log probability 进行排序，最大可能性的 label 即为预测结果\n",
    "        log_sorted=sorted(res.items(),reverse=True,key=lambda x:x[1])\n",
    "        pred = log_sorted[0][0]\n",
    "        return {'prediction':pred,'log_prob':log_sorted}\n",
    "    \n",
    "    def test(self,test_set,beta=1):\n",
    "        # 在验证集上进行评估\n",
    "        # 总 label 数\n",
    "        n=self.label_freq.B()\n",
    "        # label 列表\n",
    "        labels=list(self.label_freq.keys())\n",
    "        # 测试集文本数\n",
    "        sum_=len(test_set.index)\n",
    "        # 总混淆矩阵\n",
    "        confusion_matrix=pd.DataFrame(np.zeros((n,n)),index=labels,columns=labels)\n",
    "        # 各类别的混淆矩阵\n",
    "        confusion_matrix_each_class=dict()\n",
    "        # 统计每次预测结果，记录于总混淆矩阵\n",
    "        for text_idx in test_set.index:\n",
    "            label = test_set.loc[text_idx,'label']\n",
    "            pred = self.__call__(test_set.loc[text_idx,'words'])['prediction']\n",
    "            confusion_matrix.loc[pred,label]+=1\n",
    "        confusion_matrix=confusion_matrix.astype('int64')\n",
    "        # 计算各类别混淆矩阵\n",
    "        for label in labels:\n",
    "            tp=confusion_matrix.loc[label,label]\n",
    "            fp=confusion_matrix.loc[label].sum()-tp\n",
    "            fn=confusion_matrix.loc[:,label].sum()-tp\n",
    "            tn=sum_-tp-fp-fn\n",
    "            confusion_matrix_each_class[label]=pd.DataFrame([[tp,fp],[fn,tn]],index=['+','-'],columns=['+','-']).astype('int64')\n",
    "        # 防止出现除以 0 出现 nan\n",
    "        def na2zero(x):\n",
    "            if np.isnan(x):\n",
    "                return 1\n",
    "            return x\n",
    "        # macro precision，recall\n",
    "        precision_macro=sum(na2zero(matrix.loc['+','+']/matrix.loc['+',:].sum()) for matrix in confusion_matrix_each_class.values())/n\n",
    "        recall_macro=sum(na2zero(matrix.loc['+','+']/matrix.loc[:,'+'].sum()) for matrix in confusion_matrix_each_class.values())/n\n",
    "        confusion_matrix_micro=sum(confusion_matrix_each_class.values())\n",
    "        # micro precision，recall\n",
    "        pecision_micro = na2zero(confusion_matrix_micro.loc['+','+']/confusion_matrix_micro.loc['+',:].sum())\n",
    "        recall_micro = na2zero(confusion_matrix_micro.loc['+','+']/confusion_matrix_micro.loc[:,'+'].sum())\n",
    "        # 防止 precision,recall 均为0，无法计算 F1\n",
    "        if precision_macro==0 and recall_macro==0:\n",
    "            F1_macro=0\n",
    "        else:\n",
    "            F1_macro=(1+beta**2)*precision_macro*recall_macro/(beta**2*precision_macro+recall_macro)\n",
    "        if pecision_micro==0 and recall_micro==0:\n",
    "            F1_micro=0\n",
    "        else:     \n",
    "            F1_micro=(1+beta**2)*pecision_micro*recall_micro/(beta**2*pecision_micro+recall_micro)\n",
    "        return {\n",
    "            'precision_macro':precision_macro,'recall_macro':recall_macro,\n",
    "            'precision_micro':pecision_micro,'recall_micro':recall_micro,\n",
    "            'F1_macro':F1_macro,'F1_micro':F1_micro,\n",
    "            'confusion_matrix':confusion_matrix,'confusion_matrix_each_class':confusion_matrix_each_class,\n",
    "            'confusion_matrix_micro':confusion_matrix_micro\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KZvSzOkZkN_s",
   "metadata": {
    "id": "KZvSzOkZkN_s"
   },
   "source": [
    "### 将数据集分为训练集和测试集（10%），训练集有 30%、50%、70%、90% 四个版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ptnYi6aOkN_t",
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "aborted",
     "timestamp": 1648974073544,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "ptnYi6aOkN_t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_set 0.30: 3000\n",
      "Length of train_set 0.50: 5000\n",
      "Length of train_set 0.70: 7000\n",
      "Length of train_set 0.90: 9000\n",
      "Length of test_set 0.10: 1000\n"
     ]
    }
   ],
   "source": [
    "train_rate=[0.3,0.5,0.7,0.9]\n",
    "test_rate=0.1\n",
    "np.random.seed(0)\n",
    "arr = np.arange(len(df))\n",
    "np.random.shuffle(arr)\n",
    "train_set={}\n",
    "for rate in train_rate:\n",
    "    train_set[rate] = df[['label','words']].iloc[arr[:int(len(arr)*rate)]]\n",
    "    print(\"Length of train_set %.2f:\"%rate, len(train_set[rate]))\n",
    "test_set = df[['label','words']].iloc[arr[-int(len(arr)*test_rate):]]\n",
    "print(\"Length of test_set %.2f:\"%test_rate, len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dP4RIa2ikN_t",
   "metadata": {
    "id": "dP4RIa2ikN_t"
   },
   "source": [
    "### 用 NaiveBayes 模型在四种大小的训练集上训练，并用测试集测试\n",
    "由于这部分占内存较小，所以不删除模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "uYVxFE0YkN_t",
   "metadata": {
    "executionInfo": {
     "elapsed": 91,
     "status": "aborted",
     "timestamp": 1648974073547,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "uYVxFE0YkN_t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models_NB=defaultdict(dict)\n",
    "for rate in train_rate:\n",
    "    models_NB[rate]['model']=NaiveBayes(vocab)\n",
    "    # 模型在训练集上训练\n",
    "    models_NB[rate]['model'].fit(train_set[rate])\n",
    "    # 在测试集上进行评估\n",
    "    models_NB[rate]['test_results']=models_NB[rate]['model'].test(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--jNzaI8kN_t",
   "metadata": {
    "id": "--jNzaI8kN_t"
   },
   "source": [
    "### 输出在测试集上的评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cqLkY2lNkN_u",
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "aborted",
     "timestamp": 1648974073548,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "cqLkY2lNkN_u",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.use 30% of documents:\n",
      "\n",
      "macro precision: 0.91,\tmacro recall: 0.64,\tmacro F1-score: 0.75\n",
      "micro precision: 0.88,\tmicro recall: 0.88,\tmicro F1-score: 0.88\n",
      "\n",
      "Confusion matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Film</th>\n",
       "      <th>Politician</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Book</th>\n",
       "      <th>Software</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Food</th>\n",
       "      <th>Animal</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writer</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist  Film  Politician  Writer  Book  Software  Actor  Food  \\\n",
       "Artist          31     0           0       0     0         0      0     0   \n",
       "Film             5   290           2       7    19         4      6     1   \n",
       "Politician       3     0         379      12     9         2      0     1   \n",
       "Writer           6     0           2      61    11         0      1     0   \n",
       "Book             0     1           2       3    63         0      0     3   \n",
       "Software         0     0           0       0     0        25      0     0   \n",
       "Actor            0     0           0       0     0         0      0     0   \n",
       "Food             0     0           0       0     0         0      0    12   \n",
       "Animal           0     0           0       0     0         0      0     0   \n",
       "Disease          0     0           0       0     0         0      0     1   \n",
       "\n",
       "            Animal  Disease  \n",
       "Artist           0        0  \n",
       "Film             0        0  \n",
       "Politician       0        1  \n",
       "Writer           0        0  \n",
       "Book             4        2  \n",
       "Software         0        0  \n",
       "Actor            0        0  \n",
       "Food             0        0  \n",
       "Animal           0        0  \n",
       "Disease          8       23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "2.use 50% of documents:\n",
      "\n",
      "macro precision: 0.91,\tmacro recall: 0.70,\tmacro F1-score: 0.79\n",
      "micro precision: 0.91,\tmicro recall: 0.91,\tmicro F1-score: 0.91\n",
      "\n",
      "Confusion matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Film</th>\n",
       "      <th>Politician</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Book</th>\n",
       "      <th>Software</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Food</th>\n",
       "      <th>Animal</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>2</td>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writer</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist  Film  Politician  Writer  Book  Software  Actor  Food  \\\n",
       "Artist          37     0           0       0     0         0      0     0   \n",
       "Film             2   287           1       5    10         1      6     0   \n",
       "Politician       2     0         376       6     7         0      0     0   \n",
       "Writer           4     0           4      67    13         0      1     0   \n",
       "Book             0     4           3       5    71         0      0     2   \n",
       "Software         0     0           0       0     0        30      0     0   \n",
       "Actor            0     0           0       0     0         0      0     0   \n",
       "Food             0     0           0       0     0         0      0    15   \n",
       "Animal           0     0           0       0     0         0      0     0   \n",
       "Disease          0     0           1       0     1         0      0     1   \n",
       "\n",
       "            Animal  Disease  \n",
       "Artist           0        0  \n",
       "Film             0        0  \n",
       "Politician       0        1  \n",
       "Writer           0        0  \n",
       "Book             1        2  \n",
       "Software         0        0  \n",
       "Actor            0        0  \n",
       "Food             0        0  \n",
       "Animal           0        0  \n",
       "Disease         11       23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "3.use 70% of documents:\n",
      "\n",
      "macro precision: 0.91,\tmacro recall: 0.71,\tmacro F1-score: 0.80\n",
      "micro precision: 0.91,\tmicro recall: 0.91,\tmicro F1-score: 0.91\n",
      "\n",
      "Confusion matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Film</th>\n",
       "      <th>Politician</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Book</th>\n",
       "      <th>Software</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Food</th>\n",
       "      <th>Animal</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>2</td>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writer</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist  Film  Politician  Writer  Book  Software  Actor  Food  \\\n",
       "Artist          37     0           0       0     0         1      0     0   \n",
       "Film             2   287           1       5     6         0      6     0   \n",
       "Politician       1     0         375       5     7         0      0     0   \n",
       "Writer           5     0           5      66    11         0      1     0   \n",
       "Book             0     4           3       7    77         0      0     0   \n",
       "Software         0     0           0       0     0        30      0     0   \n",
       "Actor            0     0           0       0     0         0      0     0   \n",
       "Food             0     0           0       0     0         0      0    17   \n",
       "Animal           0     0           0       0     0         0      0     0   \n",
       "Disease          0     0           1       0     1         0      0     1   \n",
       "\n",
       "            Animal  Disease  \n",
       "Artist           0        0  \n",
       "Film             0        0  \n",
       "Politician       0        1  \n",
       "Writer           0        0  \n",
       "Book             0        2  \n",
       "Software         0        0  \n",
       "Actor            0        0  \n",
       "Food             0        0  \n",
       "Animal           0        0  \n",
       "Disease         12       23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "4.use 90% of documents:\n",
      "\n",
      "macro precision: 0.90,\tmacro recall: 0.71,\tmacro F1-score: 0.80\n",
      "micro precision: 0.91,\tmicro recall: 0.91,\tmicro F1-score: 0.91\n",
      "\n",
      "Confusion matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Film</th>\n",
       "      <th>Politician</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Book</th>\n",
       "      <th>Software</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Food</th>\n",
       "      <th>Animal</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Film</th>\n",
       "      <td>2</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writer</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist  Film  Politician  Writer  Book  Software  Actor  Food  \\\n",
       "Artist          37     0           0       0     0         1      0     0   \n",
       "Film             2   286           1       5     4         0      5     0   \n",
       "Politician       0     0         375       4     7         0      0     0   \n",
       "Writer           6     0           5      66    11         0      2     0   \n",
       "Book             0     5           3       8    79         0      0     0   \n",
       "Software         0     0           0       0     0        30      0     0   \n",
       "Actor            0     0           0       0     0         0      0     0   \n",
       "Food             0     0           0       0     0         0      0    17   \n",
       "Animal           0     0           0       0     0         0      0     0   \n",
       "Disease          0     0           1       0     1         0      0     1   \n",
       "\n",
       "            Animal  Disease  \n",
       "Artist           0        0  \n",
       "Film             0        0  \n",
       "Politician       0        1  \n",
       "Writer           0        0  \n",
       "Book             0        2  \n",
       "Software         0        0  \n",
       "Actor            0        0  \n",
       "Food             0        0  \n",
       "Animal           0        0  \n",
       "Disease         12       23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for rate in train_rate:\n",
    "    i+=1\n",
    "    print('%d.use %2.f%% of documents:\\n'%(i,rate*100))\n",
    "    print('macro precision: %.2f,\\tmacro recall: %.2f,\\tmacro F1-score: %.2f'%(models_NB[rate]['test_results']['precision_macro'],models_NB[rate]['test_results']['recall_macro'],models_NB[rate]['test_results']['F1_macro']))\n",
    "    print('micro precision: %.2f,\\tmicro recall: %.2f,\\tmicro F1-score: %.2f'%(models_NB[rate]['test_results']['precision_micro'],models_NB[rate]['test_results']['recall_micro'],models_NB[rate]['test_results']['F1_micro']))\n",
    "    print('\\nConfusion matrix:\\n')\n",
    "    display(models_NB[rate]['test_results']['confusion_matrix'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着训练数据的增加，可以看到macro、micro的准确率、召回率、F1-score均在逐步提高。根据 confusion matrix ，模型在预测 actor 和 animal 时表现极差，常常把 actor 预测为 film（可能是因为二者本来就有很强的内在联系），把 animal 预测为 disease（猜想是因为动物出现的场景下，常常会带来疾病。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uJaF940mwbhG",
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "aborted",
     "timestamp": 1648974073549,
     "user": {
      "displayName": "顾淳",
      "userId": "16399324903545072138"
     },
     "user_tz": -480
    },
    "id": "uJaF940mwbhG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352.41s\n"
     ]
    }
   ],
   "source": [
    "time2=time.time()\n",
    "print('%.2fs'%(time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整版 kneser_ney 实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngrams_model:\n",
    "    def __init__(self, n, method, vocab):\n",
    "        self.n = n\n",
    "        # 预定义 self.dic 中的每个 value 均为 ConditionalFreqDist 类， self.dic 第一层选择 n-gram（应对需要回退或者插值的情况），\n",
    "        # 第二层选择 context，第三层选择 word\n",
    "        self.prefix = defaultdict(ConditionalFreqDist)\n",
    "        # unigram 不需要 context\n",
    "        self.uni = FreqDist()\n",
    "        self.vocab = vocab\n",
    "        if method in ['add_one', 'kneser_ney']:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise 'Method is restricted in `Add-one` or `Kneser-Ney`'\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(sent, k):\n",
    "        if k==1:\n",
    "            return sent+['<end>']\n",
    "        pad1 = ['_'] * (k - 1) + ['<begin>']\n",
    "        pad2 = ['<end>']\n",
    "        return pad1 + sent + pad2\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # 得到各个长度的语言模型上的 context 和 word 的对应关系\n",
    "        if self.method == 'add_one':\n",
    "            k = self.n\n",
    "            for text in dataset:\n",
    "                for sent in text:\n",
    "                    # 在每句话前后加 padding\n",
    "                    sent = ngrams_model.pad(sent, k)\n",
    "                    for i in range(len(sent) - k + 1):\n",
    "                        gram = tuple(sent[i:i + k])\n",
    "                        if k == 1:\n",
    "                            self.uni[gram[0]] += 1\n",
    "                        else:\n",
    "                            self.prefix[k][gram[:-1]][gram[-1]] += 1\n",
    "        elif self.method == 'kneser_ney':\n",
    "            self.suffix = defaultdict(ConditionalFreqDist)\n",
    "            self.total_gram=defaultdict(int)\n",
    "            self.middle=defaultdict(ConditionalFreqDist)\n",
    "            for k in range(self.n,0,-1):\n",
    "                for text in dataset:\n",
    "                    for sent in text:\n",
    "                        # 在每句话前后加 padding\n",
    "                        sent = ngrams_model.pad(sent, k)\n",
    "                        for i in range(len(sent) - k + 1):\n",
    "                            gram = tuple(sent[i:i + k])\n",
    "                            if k == 1:\n",
    "                                self.uni[gram[0]] += 1\n",
    "                            else:\n",
    "                                self.prefix[k][gram[:-1]][gram[-1]] += 1\n",
    "                                self.suffix[k][gram[1:]][gram[0]] += 1\n",
    "                            if k>=3:\n",
    "                                # 为后面计算 C_kn 做准备\n",
    "                                self.middle[k][gram[1:-1]][(gram[0],gram[-1])]=1\n",
    "                self.total_gram[k] = sum(dic.B() for dic in self.prefix[k].values())\n",
    "\n",
    "    def add_one(self, context, word):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            dic = self.uni\n",
    "        else:\n",
    "            dic = self.prefix[n][context]\n",
    "        # 进行 add-1 平滑\n",
    "        up = dic[word] + 1\n",
    "        down = dic.N() + len(self.vocab)\n",
    "        return up / down\n",
    "\n",
    "    def kneser_ney(self, context, word, d=0.1):\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            # P_continuation()\n",
    "            up = self.suffix[2][(word,)].B()\n",
    "            down = self.total_gram[2]\n",
    "            res = (up + 1) / (down + len(self.vocab))\n",
    "            return res\n",
    "        if n == self.n:\n",
    "            dic = self.prefix[n][context]\n",
    "            if dic[word]:\n",
    "                a = max(0, dic[word] - d) / dic.N()\n",
    "            else:\n",
    "                a = 0\n",
    "            if dic.B() == 0:\n",
    "                lam = 1\n",
    "            else:\n",
    "                lam = d * dic.B() / dic.N()\n",
    "        else:\n",
    "            # Continuation count(x)= Number of unique single word contexts for x\n",
    "            context_plus = tuple(list(context) + [word])\n",
    "            c_kn_up = self.suffix[n + 1][context_plus].B()\n",
    "            c_kn_down = self.total_gram[n+1]\n",
    "            if c_kn_up:\n",
    "                a = max(0, c_kn_up - d) / c_kn_down\n",
    "            else:\n",
    "                a = 0\n",
    "            # 计算 q，使得对 w_i 求和为 1\n",
    "            q = self.middle[n+1][context[1:]].B()\n",
    "            if q == 0:\n",
    "                lam = 1\n",
    "            else:\n",
    "                lam = d*q/c_kn_down\n",
    "        return a + lam * self.kneser_ney(context[1:], word, d)  # 递归\n",
    "\n",
    "    def perplexity(self, sent):\n",
    "        # 计算困惑度\n",
    "        # 如果输入为 string，对其进行 preprocessing 以及分词\n",
    "        if isinstance(sent, str):\n",
    "            sent = del_blank(word_tokenize(preprocess(sent)))\n",
    "        sent_ = ngrams_model.pad(sent, self.n)\n",
    "        ngrams_ = [tuple(sent_[i:i + self.n]) for i in range(len(sent_) - self.n + 1)]\n",
    "        # 计算 log probability，防止数据下溢\n",
    "        log_prob = -sum([log(getattr(self, self.method)(gram[:-1], gram[-1]), 2) for gram in ngrams_]) / len(sent)\n",
    "        return pow(log_prob, 2)  # 由 log probabilitu 转换回 probability\n",
    "\n",
    "    def test(self, dataset):\n",
    "        # 计算测试集各句子平均困惑度\n",
    "        res = []\n",
    "        for text in dataset:\n",
    "            for sent in text:\n",
    "                res.append(self.perplexity(sent))\n",
    "        return sum(res) / len(res)\n",
    "\n",
    "    def generate(self, early_stop=None):\n",
    "        # 生成句子\n",
    "        # 设置初始 context\n",
    "        context = tuple(['_'] * (self.n - 2) + ['<begin>'])[:self.n - 1]\n",
    "        gram = '<begin>'\n",
    "        sent = []\n",
    "        keys = list(self.vocab)\n",
    "        i = 0\n",
    "        while gram != '<end>':\n",
    "            # 获得一个 [0,1] 的概率\n",
    "            p = np.random.rand()\n",
    "            # 获得词汇表中所有词的概率 pdf\n",
    "            pdf = []\n",
    "            for key in keys:\n",
    "                pdf.append(getattr(self, self.method)(context, key))\n",
    "            # 累加得到 cdf\n",
    "            cdf = np.array(pdf).cumsum()\n",
    "            # 找到随机概率对应的 word\n",
    "            gram = keys[cdf.searchsorted(p)]\n",
    "            sent.append(gram)\n",
    "            context = tuple(list(context[1:]) + [gram])\n",
    "            i += 1\n",
    "            # 是否需要提前停止\n",
    "            if early_stop is not None and i >= early_stop:\n",
    "                sent.append('<end>')\n",
    "                break\n",
    "        # 去除最后的 <end> \n",
    "        sent = sent[:-1]\n",
    "        return sent\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # 可直接调用模型计算单词概率\n",
    "        return getattr(self, self.method)(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
